---
title: "PC Practical: Linear Regression I"
output: html_document
---

## Exercise 16

For 96 fish (dojo loaches and goldfish) the resistance against the poison EI-43,064 was tested by placing each fish separately in a barrel with 2 liters of water and a certain dose (in mg) of the poison. Next to the survival time in minutes (the outcome, ‘minsurv’) the weight of the fish (in gram) was measured. The research questions are ‘What is the association between dose and survival time?’, ‘Will, for the same dose, a higher weight of the fish yield a higher survival time?’, ‘Are dojo loaches more resistant to the poison?’, and finally ‘Is the effect of the dose on the survival time different for dojo loaches and goldfish?’. We will try to answer these questions in this practical.

First read in the dataset poison.dat.
Read in the dataset via `read.csv`. Change the directory to the folder where `poison.dat` is saved.
```{r}
poison <- read.csv("poison.dat", sep="")
attach(poison)
```

### Problem 1: simple linear regression

**Simple Linear Regressions is regression where the dependent variable is modelled in function of only 1 independent variable, i.e., linear regression of the form $E(Y_i) = \beta_0 + \beta_1X_i$ (here $E$ stands for the expected value). In this representation $Y$ is the dependent variable (or response) and $X$ the independent variable (or predictor).**

### In this part we will try to answer the first research question "What is the association between dose and survival time?"

#### 1. Make an appropriate plot to look at the association between dose and survival time

Verify whether it is realistic to assume a linear association between dose and survival time by first drawing the best fitting curve to the point cloud and then also the best fitting line.
```{r}
plot(x = dose, y = minsurv, xlab = "Dose", ylab = "Survival time")
# Fit a curve through the points cloud (solid line)
lines(lowess(dose, minsurv), lty = 1)  # lty = 1 draws a solid line
# Fit a line through the point cloud using least squares method (dotted line)
abline(lsfit(dose, minsurv), lty = 2) # lty = 2 draws a dotted line, If this option is not specified, a solid line is drawn
```

The curve approximates relatively well a line. Based on the figure, we can conclude that it is realistic to assume a linear association between `minsurv` and `dose`.

#### 2. Postulate, based on the previous plot, a meaningful model for the mean survival time in function of the dose

Model:

$minsurv_i = \beta_0 + \beta_1 dose_i + \epsilon_i$

or

$E\{minsurv_i\} = \beta_0 + \beta_1 dose_i$

where $E$ stands for the expected value.


#### 3. Perform a linear regression analysis to estimate the parameters in the model.

```{r}
# Fit a linear regression model using 'minsurv' as dependent and 'dose' as dependent variable

# To obtain only the estimated coefficients of the linear regression model, use the command
model <- lm(minsurv ~ dose) 
# If, next to that, you would also like to know (among others) the standard errors and p-values of these estimates, use the command
summary(model)
```

**Assumptions of linear regression:**

 - Independent observations
 - Normally distributed residuals
 - Linearity between response and predictor (implies that the residuals are distributed around zero)
 - Homoscedasticity

##### Verify at the same time the assumptions that are needed to perform this analysis and trust its results.

##### a) Is the assumption of linearity realistic? This can be verified in 2 ways. The first one is by using the scatterplot we made in question 1. If the curve is more or less linear, you can conclude that the linearity assumption seems realistic. A better way is to make a residual plot and check if the points in this plot are randomly scattered around the line Y = 0. You cannot see any relation between the fitted values (or the predictor values) and the residuals. A residual plot is a scatterplot with on the X-axis the fitted values (or predictor values), and on the Y -axis the residuals. These values can be retrieved by the commands *fitted(model)* and *resid(model)*

```{r}
# Assumptions OK?
# a. Linearity: 
# A scatterplot of these values, with a horizontal line through 0 (h=0) and the best fitting curve, is obtained by
plot(fitted(model), resid(model))
lines(lowess(fitted(model), resid(model)))
abline(h = 0, lty = 2) 
```

If the curve approximates the horizontal line relatively well, the can conclude that the linearity assumption holds.

##### b) Is the assumption of homoscedasticity met? 
To verify this, we make a scatterplot of the squared residuals in function of the fitted values (or predictor values):
*plot(fitted(model), resid(model)ˆ2)*

```{r}
# b. Homoscedasticity: doubtful...
plot(fitted(model), resid(model)^2)
standardisedResiduals = resid(model)/summary(model)$sigma
plot(x = fitted(model), y = sqrt(abs(standardisedResiduals)))
lines(lowess(x = fitted(model), y = sqrt(abs(standardisedResiduals))), col = "red")
```

##### c) Are the residuals normally distributed? This will be verified using a QQ-plot for the residuals

```{r}
# c. Normally distributed residuals: 
qqnorm(resid(model))
qqline(resid(model)) # long tail on right + short tail on left. Not really OK
```

Plot all diagrams together:
```{r}
# Checking multiple assumptions simultaneously for a model:
par(mfrow=c(2,2))
plot(model)
``` 

**The assumption of homoscedasticity is doubtful and the residuals do not follow a normal distribution. We could transform the dependent variable in hope of meeting the assumptions after all.**

#### 4. If not all the previous assumptions hold, an appropriate transformation of either the dependent or the independent variable has to be applied and a linear regressions analysis on the transformed variable(s) has to be performed. Verify if here all the assumptions hold. 

Try for yourself certain transformations of the dependent `minsurv` variable, refit a linear regression model and check if the assumptions seem more plausible. Common transformations are the square root, the inverse and the logarithmic transformation.

```{r}
model_log <- lm(log(minsurv) ~ dose)
par(mfrow = c(2, 2))
plot(model_log)
summary(model_log)

minsurv_inv <- 1 / minsurv
model_inv <- lm(minsurv_inv ~ dose)
par(mfrow = c(2, 2))
plot(model_inv)
summary(model_inv)
```

#### 5. If all assumptions hold, write down the final model for the mean outcome with the estimated regression coefficients and interpret this model (both on the scale of the transformed values as on the original scale).

**For the log-transformation:**

Interpretation on the scale of the transformed values:

$log(minsurv_i) = \hat{\beta}_0 + \hat{\beta}_1 dose_i + \epsilon_i$

$log(minsurv_i) = 2.105 - 0.5112 dose_i + \epsilon_i$

with $\epsilon_i \sim N(0,0.5104^2)$.

If the dose increases with 1 miligram, then the mean of the logarithm of the survival time decreases with 0.5112.


Interpretation on the original scale:

Due to the transformation, we model the mean of the logarithm of the response variable, and we interpret the parameters in this context, as above. If we take the exponential of the estimated parameters, then these apply in terms of the geometric mean of the response variable on the original scale, since the geometric mean of $Y$ is equal to

$exp\{n^{-1} \sum_{i=1}^n log(y_i)  \}.$

Remark that the `log` transformation in the statistic typically indicates the natural logarithm (i.e., with base $e$ and not with base $10$). This way we then also interpret the parameters:

$\widehat{minsurv}_i = exp(2.105 - 0.5112 dose_i) = \frac{e^{2.105}}{e^{0.5112 dose}}$

Therefore, if the dose of the poison increases with one miligram, then the geometric mean of the survival time decreases with a **factor** of
$1/e^{0.5112} = 0.60$.

To clarify this interpretation, do the following:

 - Calculate the expected geometric mean of the survival time for a dose of 1 miligram
 - Do the same for a dose of 2 miligram
 - Compare these values. What do you notice?


#### 6. Give a 95% confidence interval for the effect of the dose on the expected outcome and test if this effect is significantly different from 0 at the 5% significance level. In R we retrieve this interval by *condint(model)*

```{r}
log.minsurv <- log(minsurv)
logModel <- lm(log.minsurv ~ dose)
confint(logModel) # 95% CI
summary(logModel) # p-value for tests vs 0
```

Note the link between CI and p-value!

#### 7. Perform a variance analysis and interpret the obtained p-value
```{r}
anova(logModel)
```

**How do we interpret the p-value?**

The residual sum of squares is significantly reduced when adding dose as an independent variable, compared with only an intercept (the overall mean).

#### 8. Estimate the geometric mean of the survival time for a dose of 2 mg. Give the accompanying 95% confidence interval.
```{r}
# To obtain this confidence interval, you first create a new dataset (call this poison2) with 1 variable that you call ‘dose’ for which you enter in observation, namely 2.

poison2 <- data.frame(dose = c(2))
# Prediction on the log scale: predict mean(log(y))
predict(logModel, poison2, interval = "c") # interval = "c" indicates we want to obtain confidence intervals

# Geometric mean: exp(mean(log(y)))
exp(predict(logModel, poison2, interval = "c")) 
```

Remark that $e^{0.9122891}=2.490016$ and $e^{1.252174}=3.497941$.

#### 9. What can we conclude from the multiple correlation coefficient?
```{r}
summary(logModel)
```

The dose variable (i.e., the regression line) succeeds in explaining 10.88% of the total variation in the response variable $Y$.


#### 10. Perform an appropriate test to see is the dose effect is nonlinear (e.g., quadratic). To this end we first create a new variable containing the squares of the doses and then add this variable as a predictor to the linear regression model we already had.
```{r}
sqdose <- dose^2
logModel2 <- lm(log.minsurv ~ dose + sqdose)
summary(logModel2)
```

**Is it useful to add a quadratic effect?**


#### 11. Perform for this new model again a variance analysis and interpret the p-value
```{r}
anova(logModel2)
```
Performing ANOVA on a multiple regression model will **sequentially** test the predictors.

p-value for `dose`: "Does the variable dose explain a significant proportion of the variation in the response variable?"

p-value for `I(dose^2)`: "Does the square of the variable dose explain a significant proportion of the variation in the response variable **given that the variable dose is already included in the model**?"

The effect of the squared dose on the mean logarithm of the survival time in minutes for dojo loaches and goldfish is **not significantly** different from zero at the **5\% significance level (p = 0.63)**, given that the dose effect is already present in the model. Adding the squared dose effect does not contribute significantly to explaining the variation in survival time, given that the dose effect is already present in the model.


### Problem 2: multiple linear regression

In this problem we will construct a multivariate linear regression model to formulate an answer for the remaining research questions ‘Will, for the same dose, a higher weight of the fish yield a higher survival time?’, ‘Are dojo loaches more resistant to the poison?’, and ‘Is the effect of the dose on the survival time different for dojo loaches and goldfish?’.

#### 1. Fit 2 regression models: 1 with the dose and weight and 1 with dose and the type of fish. Check which variable (`weight` or `type`) has the largest $t$-value and keep this variable in the model, on condition that that variable is significantly associated with the outcome. Then verify whether the other variable has an additional significant effect.

```{r}
log.minsurv <- log(minsurv)
modela <- lm(log.minsurv ~ dose + weight)
modelb <- lm(log.minsurv ~ dose + type)
summary(modela) 
summary(modelb) 
```

Variable `weight` has the largest $t$-value. We keep `weight` in the model because it has a significant effect.

**Does `type` explain significant extra variation in the response, given that `dose` and `weight` are already in the model?**

```{r}
modelc <- lm(log.minsurv ~ dose + weight + type) 
summary(modelc)
anova(modelc)
```

Yes! The $p$-value of the ANOVA F-test has value $3 * 10^{-12}$. The effect of of `type` on the survival time is highly significantly ($p < 0.001$) different from 0 at the 5\% significance level, given that `dose` and `weight` are already in the model. Or differently phrased: given that `dose` and `weight` are already in the model, the addition of the `species` effect contributes highly significantly to explaining the variation in survival time.

__What is the difference between the p-value from summary(modelc) and anova(modelc)__

The p-values fro "summary" need to be interpreted as follows: does the corresponding predictor variable explain a significant part of the variation in the response variable, given that all other predictors are already included in the model? Or else: given the model structure, is the effect of the predictor variable on the mean of the response variable significantly different from zero?

The p-values from "anova" are interpreted as: does the corresponding predictor variable explain a significant part of the response variable given that the predictor variables that are above it are already in the model? Or else: given that the predictors above this one are already included in the model, is the effect of this predictor variable on the mean of the response variable significantly different from zero?

Remark that the $p$-value for `type` is the same in both cases (using "summary" and "anova"), while this is not the case for `dose` en `weight`. This is because the p-value for `type` in both cases is conditional on the presence of both the `dose` and `type`-effect in the model.

For summary:

- the $p$-value for `dose` is conditional on the presence of the intercept, the `weight`-effect and the `type`-effect in the model. 
- the $p$-value for weight is conditional on the presence of the intercept, the `dose`-effect and the `type`-effect in the model. 
- the $p$-value for type is conditional on the presence of the intercept, the `dose`-effect and the `weight`-effect in the model. 

For anova:

- the $p$-value for `dose` is conditional on the presence of the intercept in the model. 
- the $p$-value for `weight` is conditional on the presence of the intercept,
and the `dose` effect in the model. 
- the $p$-value for `type` is conditional on the
presence of the intercept, the `dose` effect and the `weight` effect in the
model.


#### 2. Investigate if there are significant higher order terms (interactions and/or quadratic effects). 

**Is a quadratic effect for weight needed?**

```{r}
sqrweight <- weight^2
modeld <- lm(log.minsurv ~ dose + weight + sqrweight + type)
sqrdose <- dose^2
modele <- lm(log.minsurv ~ dose + sqrdose + weight + type)
summary(modeld)
summary(modele)
```

No, there is no significant effect of the square of the weight on the survival time (at the 5\% significance level), given that the dose, weight and type are already in the model (p = 0.28). Adding `sqrweight` does not significantly contribute in explaining the variance in the survival time. We leave the quadratic term for weight out of the model.

**Is there an interaction between dose and type?**

```{r}
modele <- lm(log.minsurv~ dose + weight + type + dose:type)
summary(modele) #not significant -> leave interaction out of model
```

No, there is no significant effect of the interaction between dose and species on the survival time (at the 5\% significance level), given that dose and type are already in the model (p = 0.74). The addition of the interaction-effect between dose and type doesn't contribute significantly to explaining the variation in the survival time. We leave the interaction term between dose and type out of the model.

Final model:

```{r}
model_final <- lm(log.minsurv ~ dose + weight + type)
```

The model selection procedure which we used here is called "forward selection":
we add variables to the model to see if they are significant. If this is the case we leave them in the model, if not we leave them out. In every step we look at different terms and keep the most significant (highest $t$-value or smallest $p$-value).

#### 3. Report the final model with the regression coefficients and verify the assumptions made for applying this regression analysis.

**Check all the assumptions!**

```{r}
par(mfrow=c(2,2))
plot(model_final)
par(mfrow=c(1,1))
plot(x=fitted(model_final),y=resid(model_final)^2)
```

1. independent data

The same as before. The researchers tested 96 fish by placing each fish in a
barrel with 2 liters of water and a certain dose of the poison. If you assume
that the randomization has been executed correctly, you can assume independence
because in a good randomisation observations are chosen independent from each
other.

2. Linearity between response and predictor (implicates that residuals are
   distributed around zero)
   
To check the linearity assumption, we plot the values of the residuals in
function of the fitted values. There seems to be a weak quadratic relationship
between fitted values and residuals.

3. Normally distributed residuals

For this we check the QQ-plot. The residuals seem to be normally distributed
(the precentiles of the residuals correspond well with the percentiles you
expect based on the normal distribution).

4. Homoscedasticity

For this we check the graph which plots the fitted values in function of the
square root of the absolute values of the standardized residuals. If the data
are homoscedastic, then the mean of the abolute value of standardized
residuals will always be around the same value, independent of the fitted
value. A smoother through the point cloud will be almost horizontal and not
show any clear trend. Here we see the smoother is indeed almost horizontal.
Only at the higher end of the range, the smoother decreases. Probably this
only due to one data point at the higher end, so the homoscedasticity
assumption seems to be fulfilled. 

**Conclusion:** Possibly the linearity assumption is violated. Possibly, a
higher order effect is at play which is not yet included in the model. If the
model would be used to predict new observations, it is possible that values at
the ends of the range would be underestimated, and values at the centre of the
range would be overestimated. 

#### 4. Interpret the estimated parameters, in other words, describe to what extent the dose, the weight of the fish, and the type of fish have an influence on the survival time of the fish.

```{r}
summary(model_final)
```

 - a. Interpretation of the dose estimate:

For fish of **the same species** and **the same weight** the mean of the
natural logarithm of the survival time is estimated to decrease with 0.6, with
every increase of the dose of the poison with 1 mg.

 - Or after backtransformation:

For fish of **the same species** and with **the same weight**, the geometric
mean of the survival time (in minutes) is estimated to decrease with **a
factor** $e^{-0.60435} = 0.55$, with every increase of the dose of the poison with 1 mg. 

 - b. Interpretation of the weight estimate:

For fish which weigh 1 gram more than the other fish of **the same species** and
which were treated with **the same dose of poison**, the mean natural logarithm
of the survival time is estimated to be 0.82 higher.

 - Or after backtransformation:

For fish which weigh 1 gram more than other fish of the **the same species**
and which were treated with the **the same dose of poison**, the geometric mean
of the survival time (in minutes) is estimated to be **a factor**
$e^{0.82498} = 2.28$ higher.

 - c. Interpretation of the type estimate (given that 0 codes for dojo loaches
   and 1 for goldfish):

When fish of **the same weight** are treated with **the same dose of poison**,
the mean natural logarithm of the survival time is estimated to be 0.51 higher
for goldfish compared with dojo loaches.

 - Or after backtransformation:

When fish of **the same weight** are treated with **the same dose of poison**,
the geometric mean of the survival time (in minutes) is estimated to be **a
factor** $e^{0.51248} = 1.69$ higher for gold fish compared with dojo loaches. 

#### 5. What can you conclude from the multiple correlation coefficient?

The multiple correlation coefficient $R^2$ is:

$R^2 = \frac{SS_{Regression}}{SS_{Total}}$

In the final model the multiple correlation coefficient is equal to 0.73. This
means that 73\% of the variation in the data is explained by the association
with the independent variables dose, weight and type.

