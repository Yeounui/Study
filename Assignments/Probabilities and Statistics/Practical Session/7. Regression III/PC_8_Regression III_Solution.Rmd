---
title: "PC practical Linear Regression III"
Last Modified date: "17/04/2020"
output: html_document
---

## Exercise 19

In a study on the abundance of birds _(ABUNDANCE)_ in nature reserves, 56 forests were investigated. The surface area of the forest areas was measured in hectare
and reported on a natural logarithmic scale _(LNSURF)_. Also the number of years that the location was recognized as a nature reserve was recorded _(YEARS)_, as
well as the elevation of the area above sea level _(ELEVATION)_.

### 1. Check graphically the association between the abundance of birds and the (log) surface of the reserve. Explain how you proceed and interpret the graph.

```{r}
birds <- read.table("birds.txt", header=TRUE)
attach(birds)
```


```{r}
plot(x = LNSURF, y = ABUNDANCE)
lines(lowess(x = LNSURF, y = ABUNDANCE), col = 'red')
```

We can use lowess (locally weighted scatterplot smoothing), a type of local regression, to verify the linear association between 1 predictor variables and a
response variable.

**Is the assumption of linearity fulfilled?**
The lowess-line flattens at large surfaces. This could be coincidence due to the low number of observations in that region. Let us assume there is a linear
associations.

### 2. Describe this association also numerically

```{r}
model <- lm(ABUNDANCE ~ LNSURF)
summary(model)
```

### 2a. Calculate on the basis of a regression model a value which describes the association and interpret this value both on the logarithmic and original scale.

**Interpretation of the LNSURF-variable on logarithmic scale:**

In forest areas, the mean abundance of birds lies 4.25 units higher for each increase of 1 unit of the natural logarithm of the surface in ha.

**Interpretation of the LNSURF-variable on the original scale:**

- 1\% increase: $4.2467 * log(1.01) = 0.04225607$

The mean abundance of birds in forest areas increases with 0.042 units for every increase in the surface with 1%.

Recall that "log" in (and in these files) stands for the natural logarithm ("ln") and not for the decimal logarithm. The decimal logarithm is calculated with the function "log10".


### 2b. Give a 95% confidence interval for the strength of this association and interpret the result in an insightful manner

**Interpretation of the 95% confidence interval on the log-scale:**

```{r}
confint(model, level = 0.95)
```

With 95% probability, the interval 3.19 to 5.30 contains the true mean increase in abundance of birds per increase of 1 unit of the natural logarithm of the
area of the forest in ha.

**Interpretation of the 95% confidence interval on the original scale (for 1% increase in surface):**

```{r}
confint(model, level = 0.95) * log(1.01)
```

With 95% probability, the interval 0.032 tot 0.053 contains the true mean increase in abundance of birds per 1% increase in the surface of the forest.

### 3. Build a linear regression model for the mean abundance in function of the elevation of the area, the (log) surface of the area, and the number of years the area was a nature reserve. Allow for interaction and quadratic effects, if this is supported sufficiently by the data. It is not necessary to try any transformations.Report your final model that contains only statistically significant predictors (at the 5% significance level).

Explain in short your strategy to obtain this model by showing all the intermediate models you have fit. Write down these models in short, like, e.g., Y = LNSURF for the model that only contains LNSURF and Y = LNSURF + HEIGHT + LNSURF * HEIGHT which contains the main effects and an interaction between LNSURF and ELEVATION.

__Forward model selection__

**Step 1: compare all possible simple linear regression models:**

```{r}
model1a <- lm(ABUNDANCE ~ LNSURF); summary(model1a) #t = 8.086
model1b <- lm(ABUNDANCE ~ YEARS); summary(model1b) #t = 4.281
model1c <- lm(ABUNDANCE ~ ELEVATION); summary(model1c) #t = 3.073
```

We continue with model 1a because it has the absolute highest t-value (and thus lowest p-value) and the p-value for LNSURF is smaller than 0.05.

**Step 2**

```{r}
model1a <- lm(ABUNDANCE ~ LNSURF + ELEVATION); 
summary(model1a) #t = 2.138
model1b <- lm(ABUNDANCE ~ LNSURF + YEARS); 
summary(model1b) #t = 3.774
```

The variable "YEARS" has the highest t-value in absolute value. We continue with model 1b.

```{r}
model2a <- lm(ABUNDANCE ~ LNSURF + YEARS + ELEVATION); 
summary(model2a)
model2b <- lm(ABUNDANCE ~ LNSURF + YEARS + ELEVATION + I(ELEVATION^2)); 
summary(model2b)
```

**Is the effect of elevation significant?**

The elevation does not explain any significant extra variation in the response variable. The effect of the number of meters above sea level (`ELEVATION`) on
the abundance of birds in forest areas is only marginally significant (p-value = 0.09). Also the squared effect of `ELEVATION` does not explain any significant
extra variation in the response. The squared effect of the elevation is not significant (p-value = 0.39). 

**Step 3: higher order terms: are there any squared effects?**

```{r}
model3a <- lm(ABUNDANCE ~ LNSURF + I(LNSURF^2) + YEARS)
summary(model3a)
model3b <- lm(ABUNDANCE ~ LNSURF + YEARS + I(YEARS^2))
summary(model3b)
```

No.

Interactions?

```{r}
model3c <- lm(ABUNDANCE ~ LNSURF + YEARS + LNSURF:YEARS)
summary(model3c)
```

No.

Let's try adding interactions with ELEVATION, because that effect was almost significant.

```{r}
model4a <- lm(ABUNDANCE ~ LNSURF + YEARS + ELEVATION + ELEVATION:LNSURF)
summary(model4a)
```
```{r}
model4b <- lm(ABUNDANCE ~ LNSURF + YEARS + ELEVATION + ELEVATION:YEARS)
summary(model4b)
```

Final model:

```{r}
finalmodel <- lm(ABUNDANCE ~ LNSURF + YEARS + ELEVATION + LNSURF:ELEVATION)
summary(finalmodel)
```

### 4. Interpret carefully each of the estimated regression coefficients in the final model, including the intercept.

Interpretation of the parameters:

 - a. Interpretation of intercept:
 
A forest area with a surface of 1 ha (because $exp(0) = 1$) at an elevation of 0 meters above sea level and that has been recognised as natural reserver for 0
years, has a mean abundance of birds equal to -6.86.
 
  - b. Interpretation of `YEARS`:

Forest areas that have been recognised a year longer as a natural reserve than other forest areas with the **same surface** and the **same elevation above see
level** will have a mean abundance of birds which is 0.104 higher.

  - c. Interpretation of `LNSURF`:

$6.678843*log(1.01) = 0.07$

Forest areas at sea level (i.e., **elevation = 0**) that are 1% larger in surface and have the **same number of years of recognition as natural reserve**,
have a mean abundance of birds that is 0.07 higher.

 - d. Interpretation of `ELEVATION`:

Forest areas of 1 ha (i.e., **LNSURF = 0**) that lie 1 meter higher and have the **same number of years of recognition as natural reserve**, have a mean
abundance of birds that is 0.08 higher.

 - e. Interpretation of interaction between `LNSURF` and `ELEVATION`:

Forest areas with the **same number of years of recognition as natural reserve** that lie 1 meter higher, have an effect of the natural logarithm of the surface
on the mean abundance that is 0.02 lower.

Or also:

Forest areas with the **same number of years of recognition as natural reserve** that increase 1 unit in the natural logarithm of their surface, have an effect
of elevation on the mean abundance of birds that is 0.02 lower.

3rd possible interpretation:

For forest areas with the **same number of years of recognition as natural reserve**, the difference between elevation equal to 0 meters and the natural
logarithm of their surface equal to 0 versus elevation equal to 1 meter and the natural logarithm of their surface equal to 1 is associated with a decrease in
mean abundance of birds of 0.02 in addition to the individual effects of `LNSURF` and `ELEVATION`.

```{r}
range(ABUNDANCE)
range(LNSURF)
range(YEARS)
range(ELEVATION)
```

Remark that the interpretation of the intercept in this context doesn't make any sense (an abundance of -6.86 birds?!?). This value is not only far outside the
range of the model (the minimal elevation above sea level is 60 meters, so we are extrapolating here!), but is also physically impossible.

Also some main effects for which we add a higher order effect (`ELEVATION` and `LNSURF`) become difficult to interpret. For `ELEVATION` we make an
interpretation for values that are outside the range of our model (forest areas at sea level).

The interpretation of these parameters does make sense if we centered the variables around zero. Replace in that case everywhere in the interpretation the
"zero" with the mean value of that predictor value. Also remark that centering factor of dummy variables doesn't make sense. Suppose, for example, that men are
coded as "1" and women as "0" and that there are 6 men and 4 women in the dataset. A centered value of 0.6 will no longer have any meaning.

__Backward model selection__

```{r}
fullModel <- lm(ABUNDANCE ~ LNSURF + I(LNSURF^2) + YEARS + I(YEARS^2) + ELEVATION + I(ELEVATION^2) + LNSURF:YEARS + LNSURF:ELEVATION + YEARS:ELEVATION);
summary(fullModel)
```

After each time removing the least significant effect, we obtain as final model:


```{r}
step <- lm(ABUNDANCE ~ LNSURF + YEARS + ELEVATION  + LNSURF:ELEVATION); 
summary(step)
```

### 5. Investigate if the necessary assumptions are fulfilled in the obtained regression model. Describe the graphs you make and interpret these carefully.

As before:

1. Residuals are nicely distributed around zero, the linearity assumption seems to hold. The uttermost right point pulls the smoother a little down, but this
   doesn’t cause any reason for panic.

```{r}
plot(x = fitted(finalmodel), y = resid(finalmodel))
abline(h = 0, lty = 2)
lines(lowess(x = fitted(finalmodel), y = resid(finalmodel)), col = "red")
```
 
2. Homoscedasticity can be verified by plotting the squared residuals in    function of the fitted valued from the model. The red smoother connects the
   values of the fitted variance in each “area” of estimated response variables and is therefore a good estimator of the conditional variance. A trend can be
   seen in the smoother. The homoscedasticity assumption seems to be invalid.

```{r}
plot(x = fitted(finalmodel), y = resid(finalmodel)^2)
lines(lowess(x = fitted(finalmodel), y = resid(finalmodel)^2), col = "red")
```

Sometimes $\sqrt{\lvert e_i\lvert}$ in function of $\hat{y}$ is used since these values are a lot less skewed if the residuals ($e_i$) are normally distributed:

```{r}
plot(x = fitted(finalmodel), y = sqrt(abs(resid(finalmodel))))
lines(lowess(x = fitted(finalmodel), y = sqrt(abs(resid(finalmodel)))), col = "red")
```

3. Normality of the residuals seems to hold.

```{r}
qqnorm(resid(finalmodel))
qqline(resid(finalmodel))
```

4. Independence is difficult to verify, but is assumed to hold. However, if we plot the abundance of birds in function of their order in the dataset, there
   seems to exist a very strong linear association!

```{r}
plot(ABUNDANCE)
lines(lowess(x = seq(1:56), y = ABUNDANCE), col = "red")
```

This is because the dataset is already sorted from small LNSURF-values to large LNSURF-values. Always check this before assuming, based on the previous plot,
that the independence assumption is violated!

```{r}
par(mfrow = c(2, 2))
plot(finalmodel)
```

### 6. Estimate the mean abundance of birds for a reserve that has already been protected for 50 years and has a surface of 2.7 hectare, and provide the associated 95% confidence interval

```{r}
# predict(finalmodel, newdata = data.frame(LNSURF = log(2.7), YEARS = 50), interval = "confidence", level = 0.95)
```

Executing the above code would yield an error, because you also have to provide an elevation. Take, for example, an elevation of 146 meters.

```{r}
predict(finalmodel, newdata = data.frame(LNSURF = log(2.7), YEARS = 50, ELEVATION = 146), interval = "confidence", level = 0.95)
```

*The interval 14.39 to 16.41 contains, with 95% confidence, the true mean abundance of birds for a reserve that has been protected for 50 years, has a
surface of 2.7 ha, and has an elevation of 146 meters above sea level.


### 7. Investigate if there are any extreme outcome or predictor values and investigate if these values have an influence on the regression model. Describe the graphs you make for this, and/or which values you calculate, and what you conclude from them

1. Are there extreme predictor values? Look at leverage for this.

```{r}
lev = hat(model.matrix(finalmodel))
lev
```

These values are a measure for the distance of the predictor values for a specific observation in the sample to the mean of the predictor values in the
sample. 

```{r}
p <- length(coef(finalmodel))
n <- nrow(birds)
leverageTreshold <- (2*p) / n #2p/n
which(lev > leverageTreshold)
par(mfrow=c(1,1))
colors <- rep(1, nrow(birds))
colors[which(lev > leverageTreshold)] <- 2
plot(x = LNSURF, y = ABUNDANCE, type = 'p', col = colors)
plot(x = YEARS, y = ABUNDANCE, type = 'p', col = colors)
plot(x = ELEVATION, y = ABUNDANCE, type = 'p', col = colors)
``` 

**Are there observations with an extreme leverage? Which ones?**

Observation 2, 8, 52 and 56 have an extreme levarage.

Plot of the predictors with respect to each other:

```{r}
plot(x = YEARS, y = LNSURF, type = 'p', col = colors) 
plot(x = YEARS, y = ELEVATION, type = 'p', col = colors)
plot(x = ELEVATION, y = LNSURF, type = 'p', col = colors)
```

Observations with high leverage seem to be mainly extreme in terms of their elevation.

What happens if we would remove these observation with a high leverage from the model.

```{r}
birds2 <- birds[-which(lev > leverageTreshold),]
finalmodel2 <- lm(birds2$ABUNDANCE ~ birds2$LNSURF + birds2$YEARS + birds2$ELEVATION + birds2$LNSURF:birds2$ELEVATION)
summary(finalmodel2)
```

Comparing the original model with the model in which the data points with high leverage are removed:

```{r}
par(mfrow = c(1,1))
plot(x = LNSURF, y = ABUNDANCE, col = colors)

abline(a = coef(finalmodel)["(Intercept)"], b = coef(finalmodel)["LNSURF"])
abline(a = coef(finalmodel2)["(Intercept)"], b = coef(finalmodel2)["birds2$LNSURF"], col = "red")
```

2. Extreme values for the response variable: which observations have a large deviation with respect to their corresponding estimated value (i.e., have large residuals)?

**Are there extreme residuals? Which ones?**

```{r}
which(rstudent(finalmodel) > 1.959964 * summary(finalmodel)$sigma)
```

There are no extreme residuals.

```{r}
#For illustrative purposes: fitted vs. observed
plot(fitted(finalmodel), ABUNDANCE) 
lines(lowess(x = fitted(finalmodel), y = ABUNDANCE), col = "red")

#Fitted vs. residuals (gives same information as plot above)
plot(x = fitted(finalmodel), y = resid(finalmodel))
abline(h = 0, lty = 2)
lines(lowess(x = fitted(finalmodel), y = resid(finalmodel)), col = "red")
```

3. Is there an extreme influence on all fitted values? Is there an extreme influence on all coefficients?

Look at Cook's distance. Cook's distance is considered extreme if it surpasses the 50% percentile of an $F_{p,n-p}$-distribution.

```{r}
cook <- cooks.distance(finalmodel)
fquantile <- qf(0.5, df1 = p, df2 = n - p) #critical value
cook < fquantile
which(cook > fquantile) # not a single observation is extreme in terms of Cook's distance
```

There are no observations with an extreme Cook's distance (i.e., Cook’s distance larger than 0.88). Hence there are no observations with an extreme influence on
all fitted values or all coefficients.

4. Is there an extreme influence of a certain observation on certain model parameters?

The DFBETAS of the i-th coefficient are a diagnostic measure for the influence of the i-th observation on each coefficient seperately. A value of DFBETAS is
considered extreme if it surpasses 1 in small to medium datasets and $\frac{2}{\sqrt{n}}$ for large datasets.

```{r}
dfb <- dfbetas(finalmodel)
dfb > 2 / sqrt(n)
which(rowSums(dfb > 2 / sqrt(n))!=0)
```

**Which observations have an extreme influence on at least 1 of the model parameters?**

Observations 2, 11, 12, 17, 47, 52 and 56 have an extreme influence on at least 1 model parameter.


## Exercise 20

In this exercise we are studying the data of a bio-assay. A bio-assay is an experiment to determine the concentration of a substance (f.e. nitrate, penicilline, ...) in a formulation by measuring its activity in a biological system (f.e. by determining which response it has on the test animals). The data we’re going to analyse is from a study for the effect of different Vitamin D preparations on the curing against rachitis. 78 rats were given a diet without Vitamin D during 2 weeks in order them to develop rachitis. Subsequently the diet was replenished with a standard Vitamin D preparation of a certain dose, or with test preparations I till F which have to be prepared with the standard. Another 2 weeks later a radiological examination of the right knee of each animal had to determine to which degree the rachitis was cured. The radiological image was compared with a standard collection of images representing different stages of curing of the rachitis. The image of the collection which corresponds best with the observed result, is the outcome score Y. In previous studies it was shown that the outcome is linearly associated with the natural logarithm of the dose, rather than with the dose itself

The variables in the dataset assay.txt are:

* logdose: the natural logarithm of the treated dose in i.u./ml
* score: the end score (among0, 1, ..., 12) which indicated how well the rachitis was cured (the higher, the better)
* prepration: the prepration chosen (1: standard; 2: test I; 3: test F)


Read the data via `read.table`. Change your working directory to the folder where you stored `assay.dat`. 

```{r} 
assay <- read.table("assay.txt", sep="", header = TRUE) 
attach(assay)
```

#### 1. Fit a linear regression model for the mean score in function of the log dose and the preparation (store this model in an object named *model*).

```{r} 
model <- lm(score ~ logdose + preparation) 
summary(model)
head(assay)
```

**For the same dose what is the mean difference in the score between rats which were treated with preparation I and rats which were treated with the
standard?** 

When rats are treated with the same dose of vitamin D, the mean score is 0.02 higher for rats which were treated with preparation I compared with rats
which were treated with the standard. The p-value is 0.93. The effect of preparation I on the score is not signficantly different from 0 at the 5\%
significance level.

**What is for the same dose the mean difference in the score between rats which were treated with preparation F and rats which were treated with preparation I?**

When rats are treated with the same dose of vitamin D, the mean score is 0.02 higher for rats which are treated with preparation F compared with rats which
were treated with preparation I. The p-value is 0.93. The effect of preparation I on the score is not significantly different from 0 at the 5\% significance
level.

**Does this coding make sense to you?**

This coding is not useful, because the model implicitly assumes that the difference in the response variable score is 2 times bigger between preparation
F and the standard then between preparation I and the standard.

#### 2. Now make 2 dummy variables. Fit this model in R and interpret the coefficients.

Solution: dummy variables:

 - variable $prepI$ is 1 if the observation corresponds with preparation I and 0 if it doesn't.
 - variable $prepF$ is 1 if the observation corresponds with preparation F and 0 if it doesn't.

We can now relax the assumption that the score is linear for the chosen
preparation by fitting a model with logdose as the dependent variable and these
two newe variables as the independent variables.

Remark that we always work with *k*-1 dummy-variables where *k* is the
number of levels in the original factor variable. Each level of the
categorical variable is defined by a unique combination of the dummies:

 Preparation prepI prepF
 ----------- ----- -----
 standard        0     0
        I        1     0
        F        0     1

```{r}
prepI<-ifelse(preparation==2, 1, 0) #I (2): prepI is 1 if preparation==2 en 0 if not
prepF<-ifelse(preparation==3, 1, 0) #F (3): prepF is 1 if preparation==3 en 0 if not

model <- lm(score ~ logdose + prepI + prepF)
summary(model)
```

The above approach can be very inefficient if you have a lot of levels (a lot of preparations in this example), because you need to make a new variable
for every level yourself. A more efficient way is to define preparation as a factor variable and to include this factor variable in the model.
Compare the parameter estimates!

```{r}
fPreparation <- factor(preparation)
fmodel <- lm(score ~ logdose + fPreparation)
summary(fmodel)
```

**Interpret the dose variable at the original scale.**

Remark that it is the predictor variable which is log-transformed here. The
dose effect is assumed to be independent of the preparation:

Take two doses $d_1$ and $d_2$. When the predictor variable "preparation" is held constant, 

$\widehat{score}(d_1) = 0.23 + 1.31*log(d_1)+\widehat{preparation}$

and

$\widehat{score}(d_2) = 0.23 + 1.31*log(d_2)+\widehat{preparation}$

$\Rightarrow \widehat{score}(d_2)-\widehat{score}(d_1) = 1.31*(log(d_2)-log(d_1)) = 1.31*(log(d_2/d_1))$

From this we can conclude that when the percentage increase is constant, the
change in the score is constant too. F.e: 

 - 1% increase: $1.31*(log(d_2/d_1)) = 1.31*(log(1.01)) = 0.01$
 - 10% increase: $1.31*(log(d_2/d_1)) = 1.31*(log(1.1)) = 0.12$
 
So: examples of possible interpretations:

 - When rats are given the same type of preparation, the mean score will be 0.01 higher when the dose of the preparation is increased
   with 1\%.
 - When rats are given the same type of preparation, the mean score will be 0.12 higher when the dose of the preparation is increased with 10\%. 
   

#### 3a. Estimate the mean score for rats which were given a dose of 2.5 iu/mg of preparation F and give a corresponding 95\% confidence interval.

hint : For this make a new dataset, more specifically a data.frame. Make 3 variables in this dataset; the log dose, prepI and prepF (corresponding with the names of the predictors in the linear regression model), each with only 1 observation e.g. log(2.5) (calculate this first), 0 and 1 respectively.

```{r}
predict(model, newdata=data.frame(logdose=log(2.5), prepI=0, prepF=1), interval="confidence")
```

```{r}
#Or for a model with a factor variable:
predict(fmodel, newdata=data.frame(logdose=log(2.5), fPreparation="3"), interval="confidence")
```

**Interpret the 95\% confidence interval.**

With a probability of 95\% the interval 1.35 to 2.50 will contain the real mean score for rats which are treated with a dose of 2.5 iu/ml of preparation F.

#### 3b. Do you think that on the basis of the model it makes sense to estimate the mean response for rats which were treated with a dose of 1.5 iu/mg of preparation F?

```{r}
range(logdose)
log(1.5)
```

**Think about the range for which you can make predictions based on the built
model.**

A dose of 1.5 lies outside the range of the model. We can't use the model to make estimates outside of the range! At least not without making assumptions
about the regression line outside the range of the observed data...

#### 4. Interpret the multiple correlation coefficient.

```{r}
summary(fmodel)
```

**In the final model the multiple correlation coefficient is equal to 0.52.
What does this mean? What does this say about the quality of the model?**

In the final model the multiple correlation coefficient is equal to 0.52. This means that 52\% of the variation in the response variable is explained by its
association with the independent variables dose and the type of preparation.
The model doesn't have a good predictive power. But this doesn't say anything about the quality of the model!

#### 5. Make appropriate QQ-plots and residual plots to assess the final model fit. Interpret the figures and explain for each figure what any possible deviation represents. Adjust the model if necessary.


1. 
```{r}
plot(x=fitted(fmodel),y=resid(fmodel))
abline(h=0,lty=2)
lines(lowess(x=fitted(fmodel),y=resid(fmodel)),col="red")
```

2. 
```{r}
plot(x=fitted(fmodel),y=resid(fmodel)^2)
lines(lowess(x=fitted(fmodel),y=resid(fmodel)^2),col="red")
```

```{r}
#calculate the leverage
lev=hat(model.matrix(fmodel))
#Calculate the standardised residuals
standardisedResiduals = resid(fmodel)/(summary(fmodel)$sigma*sqrt(1 - lev))
plot(x=fitted(fmodel),y=sqrt(abs(standardisedResiduals)))
lines(lowess(x=fitted(fmodel),y=sqrt(abs(standardisedResiduals))),col="red")
```

Remark that the sample variance is lower for smaller values of the estimated response variable. For these small values there also is less data than for the bigger values.

In real biological data there often is heteroscedasticity, in which the variance is often larger for larger values of the response variable. This can occur for example if patients are not reacting to the same degree to the treatment, so that groups wich received a higher dose show a greater variance than groups which received a lower dose.

```{r}
table(round(fitted(fmodel),1))
```

3. 
```{r}
qqnorm(resid(fmodel))
qqline(resid(fmodel))
```

4. Independence is very difficult to investigate, but can be assumed, in this case by a good experimental design and randomization. If information about
   the score of one individual would contain information about the score of another individual, the independence assumption will not hold. This could be
   the case, for example, if certain rats would be family from each other of if the outcome would be measured multiple times for the same individuals (e.g.,
   over time). Time dependence could be verified (assuming that the data are added sequentially to the dataset) by:

```{r}
plot(score)
```

#### 6. Is ther any indication that the dose effect with use of preparation F is different with use of the standard preparation? Is there an indication that with 1 of the 3 preparations the dose effect is different than for the other 2?

Preparation I: 2, preparation F: 3

```{r}
#The model without interaction-term:
model1 <- lm(score ~ logdose + fPreparation)
summary(model1)
```

**Interpretations of the effects of preparation I and preparation F:**
 - When we compare rats which are treated with the same dose of vitamin D, the mean score is 1.37 lower for rats which were treated with preparation I compared with rats which were treated with the standard preparation. The p-value is $7*10^{-6}$. The effect of preparation I on the score is extremely significantly (p < 0.001) different from zero at the 5\% significance level.
 - When we compare rats which are treated with the same dose of vitamin D, the mean score is 0.50 higher for rats which were treated preparation F compared with rats which were treated with the standard preparation. The p-value is $0.14$. So the effect of preparation F on the score is not significantly different from 0 at the 5\% significance level. 
 
```{r}
model2 <- lm(score ~ logdose + fPreparation + logdose:fPreparation)
anova(model1,model2)
```

**Interpretation of the p-value:**

The p-value is $0.15$. The interaction-effect between the logarithm of the dose and the type of preparation is not significantly different at the 5\% significance level. There is unsufficient evidence at the 5\% significance level that for at least on of the three preparations is different from the other two preparations. More in general you can say that the more complex model (with interaction term) at the 5\% significance level doesn't explain significantly more variation in the response variable score compared with the simpler model which doesn't contain an interaction effect. 

**Remark: interaction-effects can be coded in two different ways in R.**

1. use ":" to include an interaction.

```{r}
model2b <- lm(score ~ logdose + fPreparation + logdose:fPreparation)
model2b
```

2. Use "*" to include main effects and interaction.

```{r}
model2c <- lm(score ~ logdose*fPreparation)
model2c
```

This two models are the same.

#### 7. 

Influential observations are observations which have a strong influence on the regression coefficients. Influence is a combination of large residuals and high leverage. An observation can have a large residual, but still a limited influence. An observation can also have a large leverage, and also a limited influence.

#### 7a. Are there extreme score values?

hint : We can see this on the residual plot we have made in question 5.

1. Exploration: are there extreme values in the response variable? 

```{r}
plot(score)
```

More clear:

```{r}
plot(sort(score))
```

2. Are there extreme residuals? (Plot the same as question 5).

```{r}
plot(fmodel, which=c(1))
```

Sometimes studentized residuals are plotted. Because, as a result of estimation errors, the residuals won't be perfectly normally distributed, and not have a constant variance, even when the model is correct and the response is normally distributed for fixed predictor variables, and as a homogeneous variance. Studentized residuals is a transformation of residuals which will still have a constant variance and be t-distributed with n-1 degrees of freedom (for n observations). Observations which surpass $1.96*\hat{\sigma}$ will be treated as extreme (only 5\% of the data points if the response is normally distributed for fixed predictor values).

Let's see which residuals are more extreme than $1.96*\hat{\sigma}$:

```{r}
#residuals:
which(abs(resid(fmodel)) > 1.959964*summary(fmodel)$sigma)
#studentized residuals:
which(abs(rstudent(fmodel)) > 1.959964*summary(fmodel)$sigma)
```

#### 7b. Are there extreme observations for the predictor variables for certain rats?

The leverage tells you something about extreme predictor values. Leverage measures the potential of a data point to influence the model parameters if they would be shifted with one unit in the y-direction. It is also a measure for the distance of the predictor values of the i-th observation in the sample to the mean predictor values in the sample. The leverage varies between 1/n and 1 and has a mean of p/n for all the observations. Leverages larger than 2p/n are considered extreme (with p the number of parameters and n the number of observations). 

```{r}
lev=hat(model.matrix(fmodel))
plot(sort(lev),type='h')
```

```{r}
p <- length(coef(fmodel))
n <- nrow(assay)
(2*p) / n #2p/n
which(lev > (2*p)/n) #no observations with extreme leverage
```

#### 7c. Investigate if the data for certain rats have a large influence on all the fitted values and coefficients.

The Cook's distance $D_i$ is a more direct measure to express the influence of an observation on the regression analysis. It is a measure for the influence of an observation on all predictions, or else, it is a measure for the influence of an observation on all the estimated coefficients. You can obtain the Cook's distance for an observation i by comparing every prediction $\hat{y_j}$ with the corresponding prediction $\hat{y_j(i)}$ for which observation i was not used in the fitted model. The Cook's distance is considered as extreme if it surpasses the 50\% percentile of the $F_{p,n-p}$-distribution.

Plot of the Cook's distance for every observation:
```{r}
plot(fmodel,which=c(4)) #leave-one-out diagnostic
```

```{r}
cook <- cooks.distance(fmodel)
plot(cook,type='h') #the same plot.
```

Investigate if the Cook's distance surpasses the 50\% percentile of the $F_{p,n-p}$-distribution:
```{r}
fquantile <- qf(0.5,df1 = p,df2 = n - p) #critical value
cook < fquantile
which(cook > fquantile) #none of the observations is extreme according to the Cook's distance. 
```


The DFBETA's of the i-th coefficient are a diagnostic measure for the influence the i-th observation has on each coefficient separately. This contrary to the Cook's distance, which evaluates the influence on all coefficients together. We obtain the DFBETA's for the i-th observation and the j-th coefficient $\hat{\beta_j}$ by comparing  $\hat{\beta_j}$ with the coefficient $\hat{\beta_{j(i)}}$ which we woud obtain if we would fit a regression model without the i-th observation. A value for DFBETAS is considered to be extreme if it surpasses 1 for small and moderate sized datasets and $\frac{2}{\sqrt{n}}$ for large datasets.

```{r}
dfb <- dfbetas(fmodel)
dfb > 2 / sqrt(n)
which(rowSums(dfb > 2 / sqrt(n))!=0) #which observations have a large influence on at least on 1 coefficient?
```
