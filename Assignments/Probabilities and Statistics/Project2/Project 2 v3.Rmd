---
title: "Project 2"
author: "
        Dayoung Han (01903952),
        Jaehyun koo (01715475),
        Seungchan Oh(01603277),
        Yewon Son (01507594)
        "
date: '2021 5 7'
output: html_document
---

```{r, include=FALSE}
fish <- read.csv("Fish.csv",sep = ",")
```

### **Statistical analyses**

**(a)**

>  Perform an explanatory analysis on the data. Investigate the relation between the predictor variables and the outcome Weight, taking into account the different species encoded in Species.

In the data, it consists of 6 continous variables and 1 categorical variable. The data consist of 159 observations.

  | Name   | Characteristics                    | Type       |
  | -------|:-----------------------------------| :----------|
  | Species| name of the species                | Categorical|
  | Weight | weight of individual fish (in g)   | Continuous |
  | Length1| vertical length of fish (in cm)    | Continuous |
  | Length2| diagonal length of fish (in cm)    | Continuous |
  | Length3| cross length of fish (in cm)       | Continuous |
  | Height | height of fish (in cm)             | Continuous |
  | Width  | width of fish (in cm)              | Continuous |

  There is a zero value in the data of Weight. Values of Weight are only positive real number and in fact, it indicates an unknown value. Therefore, this observation should be excluded.
  
```{r}
print(fish[41,])
fish <- fish[-41,]
attach(fish)
print(fish$Weight)
```

To identify relation between outcome and each predictor, scatterplot should be drawn.

#### **Scatterplot**

```{r, echo=FALSE}
panel.cor <- function(x, y){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- round(cor(x, y), digits=2)
    txt <- paste0("R = ", r)
    cex.cor <- 1.5
    text(0.5, 0.5, txt, cex = cex.cor)
}
# Customize upper panel
my_cols <- c("#FFC900", "#9901FF", "#FF3A0D", "#00A5F5", "#00FF02", "#0700C4", "#945DFF")  
upper.panel<-function(x, y){
  points(x,y, pch = 19, cex=0.5, col = my_cols[factor(fish$Species)])
}
# Create the plots
pairs(fish[, 2:7], 
      lower.panel = panel.cor,
      upper.panel = upper.panel)
```

#### **Log transformation**

```{r, echo=FALSE}
Weightslog <- log(fish$Weight)
Length1log <- log(fish$Length1)
Length2log <- log(fish$Length2)
Length3log <- log(fish$Length3)
Heightlog <- log(fish$Height)
Widthlog <- log(fish$Width)

pairs(cbind(Weightslog, Length1log, Length2log, Length3log, Heightlog, Widthlog), 
      lower.panel = panel.cor,
      upper.panel = upper.panel)

```
  
  For outcome Weight, each of predictors have exponential relation. Therefore, it is better to transform Weight with log for linearity. Even though transformation of predictors will not affect on distribution of outcome for predictors, it is executed for better linearity. 
  although the values of Weight are still  as values of the predictors increase, linearity for each species on each predictors are still satisfied. Residuals derived from Species will be very much corrected by dummy variable of Species.
  In addition, note that multicollinearity among Length1, Length2, and Length3 is indicated.

--------------------------------------------------------------------------------
  
**(b)**

>  Build the optimal regression model (using forward model building) which describes the relationship between Weight and the other predictors. Consider Length1, Length2, Length3, Height and Width as candidates for the independent variables. There is no need to include quadratic or higher orders effect, but interaction terms can be considered.

From **(a)**, the independent variables (predictors) and dependent variable (Weight) showed linear relationship in logarithmic form.
Hence, logarithmic form of each predictors are considered when building the model.

Candidates for the independent variables are:

* **Length1**

* **Length2**

* **Length3**

* **Height**

* **Width**

&nbsp;

##### **1. Checking for the first term.**


###### **Model1**
```{r}
model1 <- lm(Weightslog ~ Length1log); summary(model1)
```

###### **Model2**
```{r}
model2 <- lm(Weightslog ~ Length2log); summary(model2)
```

###### **Model3**
```{r}
model3 <- lm(Weightslog ~ Length3log); summary(model3)
```

###### **Model4**
```{r}
model4 <- lm(Weightslog ~ Heightlog); summary(model4)
```

###### **Model5**
```{r}
model5 <- lm(Weightslog ~ Widthlog); summary(model5)
```

All the interaction terms are significant.\
The t-value of model5 is the highest with t-value of 61.46.

Therefore, Widthlog predictor is chosen as the first term.

**model = lm(Weightslog ~ Widthlog)**

&nbsp;

##### **2. Checking for the second term.**

###### **Model1**
```{r}
model1 = lm(Weightslog ~ Widthlog + Length1log); summary(model1)
```

###### **Model2**
```{r}
model2 = lm(Weightslog ~ Widthlog + Length2log); summary(model2)
```

###### **Model3**
```{r}
model3 = lm(Weightslog ~ Widthlog + Length3log); summary(model3)
```

###### **Model4**
```{r}
model4 = lm(Weightslog ~ Widthlog + Heightlog); summary(model4)
```

All significant.\
The t-value (18.354) is highest when adding Length3log term.

Length3log is selected as the second term.

**model = lm(Weightslog ~ Widthlog + Length3log)**

&nbsp;

##### **3. Checking for the third term.**

###### **Model1**
```{r}
model1 = lm(Weightslog ~ Widthlog + Length3log + Length1log); summary(model1)
```

###### **Model2**
```{r}
model2 = lm(Weightslog ~ Widthlog + Length3log + Length2log); summary(model2)
```

###### **Model3**
```{r}
model3 = lm(Weightslog ~ Widthlog + Length3log + Heightlog); summary(model3)
```

All the interaction terms are significant.
The t-value of model3 is the highest with t-value of 12.41.

Heightlog predictor is chosen as the third term.

**model = lm(Weightslog ~ Widthlog + Length3log + Heightlog)**

&nbsp;

##### **4. Checking for the fourth term.**

# {.tabset}

###### **Model1**
```{r}
model1 = lm(Weightslog ~ Widthlog + Length3log + Heightlog + Length1log); summary(model1)
```

###### **Model2**
```{r}
model2 = lm(Weightslog ~ Widthlog + Length3log + Heightlog + Length2log); summary(model2)
```

Adding Length2log, which is model2, is the highest. (5.223)

Length2log predictor is chosen as the fourth term.

Length3log predictor became insignificant. it will stay out of the model.

**model = lm(Weightslog ~ Widthlog + Heightlog + Length2log)**

&nbsp;

##### **5. Checking for the fifth term.**
```{r}
model5321 = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Length1log); summary(model5321)
```

Addition of Length1log do not produce significant p-value.\
They will not be added to the model.

**model = lm(Weightslog ~ Widthlog + Heightlog + Length2log)**

&nbsp;
&nbsp;

Now the interaction terms will be checked.

There are 6 possible interaction terms and listed below,



* **Widthlog:Heightlog**

* **Widthlog:Length2log**

* **Heightlog:Length2log**

&nbsp;

##### **6. Checking for the first interaction terms.**




###### **Model1**
```{r}
model1 = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Widthlog:Heightlog); summary(model1)
```

###### **Model2**
```{r}
model2 = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Widthlog:Length2log); summary(model2)
```

###### **Model3**
```{r}
model3 = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log); summary(model3)
```


All the interaction terms are significant.\
The t-value of model3 is the highest with t-value of -3.374.

Heightlog:Length2log term will be added to the model.\
model3 is chosen to be added as first interaction term.

**model = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log)**

&nbsp;

##### **7. Checking for the second interaction terms.**

###### **Model1**
```{r}
model1 = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + Widthlog:Heightlog); summary(model1)
```

###### **Model2**
```{r}
model2 = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + Widthlog:Length2log); summary(model2)
```


The interaction term that gives the highest t-value (2.516) is Widthlog:Length2log.

The model2, Widthlog:Length2log term will be added to the model.

However, upon addition of Widthlog:Length2log term, Widthlog became insignificant.

As Widthlog term is included in the interaction term, although it becomes insignificant, it will stay in the model.

**model = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + Widthlog:Length2log)**

&nbsp;

##### **8. Checking for the third interaction terms.**
```{r}
model1 = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + Widthlog:Length2log + Widthlog:Heightlog); summary(model1)
```

Adding Widthlog:Heightlog term is not significant. There are no more interaction terms to be added to the model. Hence, the final model will be as below.

&nbsp;

##### **finalmodel = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + Widthlog:Length2log)**

```{r}
# Final model
finalmodel = lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + Widthlog:Length2log)
summary(finalmodel)
```

--------------------------------------------------------------------------------

**(c)**

> Add the variable Species as a predictor to your final model. Is the effect significant?\
What can you conclude?

Species are categorical variable. When adding a categorical variable, factor() function has been used.

Also, Species itself will be considered instead of changing it to log form because it is categorical variable.

```{r}
finalmodel_with_Species <- lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + Widthlog:Length2log + factor(fish$Species))
summary(finalmodel_with_Species)
```

The all factor(Species) except Whitefish terms are not significant. 

However, it is useless to see the significance of each factor term. This is because all the factors of Species have to be removed together.

Since factor(Species) of Whitefish is significant, F-test is needed.

When the factor(Species) is added to the model, Heightlog:Length2log and Widthlog:Length2log interaction terms are insignificant, these two interaction terms are going to be excluded step by step until all basic variables are significant.

##### **To test the effect of Species, the partial F-test have to be carried out.**
```{r}
mC <- lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + Widthlog:Length2log + factor(fish$Species))
mR <- lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + Widthlog:Length2log)
anova(mR, mC)
```

Deciding to add dummy variable is done through anova test by comparing the complete model with the reduced model. If it is significant, I can include the dummy variable in the model.

The p-value of the partial F-test is 0.01082.
This means that effect of Species as predictor in the model will be considered.
Species predictor is significant and does not exceeds 0.05, so it will be added to the model.

&nbsp;

#### **finalmodel = lm(Weightslog ~ Heightlog + Length2log + Heightlog:Length2log + Widthlog:Length2log + factor(Species))**

&nbsp;

##### **Checking all basic variables and interactions are significant.**

```{r}   
Modelf1 <- lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + Widthlog:Length2log + factor(fish$Species)); summary(Modelf1)
```
Width, Heightlog:Length2log and Widthlog:Length2log are insignificant. The last term, Widthlog:Length2log will be removed.

```{r}
Modelf2 <- lm(Weightslog ~ Widthlog + Heightlog + Length2log + Heightlog:Length2log + factor(fish$Species)); summary(Modelf2)
```

The Heightlog:Length2log, interaction term is insignificant, it will be removed.

```{r}
Modelf3 <- lm(Weightslog ~ Widthlog + Heightlog + Length2log + factor(fish$Species)); summary(Modelf3)
```

All the basic variables are significant with Species factor.

&nbsp;

#### **finalmodel = lm(Weightslog ~ Widthlog + Heightlog + Length2log + factor(Species))**

--------------------------------------------------------------------------------

**(d)**

> Check the assumptions of your chosen model.

To judge the final model is reliable, four conditions should be satisfied.

* **Independent data**

* **Linearity**

* **Normality**

* **Homoscedasticity**

```{r, include=FALSE}
finalmodel <- lm(Weightslog ~ Widthlog + Heightlog + Length2log + factor(Species))
```

```{r, echo=FALSE}
par(mfrow=c(2,2))
plot(finalmodel)
```

#### **Independent data**

From given data description, each observation of different fishes is from fish market sales.Here we assume that all the data are independent.

#### **Linearity between outcome and predictors** 
   
Linearity assumption is satisfied when residuals are distributed around zero. The line is vertically straight on zero, therefore linearity assumption seems to hold. 14th, 76th, and 95th observations are away from the line.

#### **Normality of residuals**

Normality of residuals are almost satisfied except for few observations are placed out of the linear line.

#### **Homoscedasticity**

The line does not have pattern meaning that the assumption of homoscedasticity is fulfilled. In addition, the observation is evenly distributed around the line, except for 14th, 95th, and 141th observation.

&nbsp;

##### **Consequently, although there are some observations which disturb the assumption of the model, the model approximately satisfies the assumption.**

--------------------------------------------------------------------------------

**(e)**

> Check if there are outliers or influential observations.

The leverage is measurements of the potential influence to model parameters if a data point be shifted with a unit in the y-direction and measurements of the distance of the predictor values of the i-th observation in the sample to the mean predictor values in the sample. The leverage varies between 1/n and 1 and has a mean of p/n for all the observations. Leverages larger than 2p/n are considered extreme


```{r}
p <- length(coef(finalmodel))   # the number of parameters
n <- nrow(fish)                # the number of observations
df <- n-p
leverageTreshold <- (2*p) / n
lev = hat(model.matrix(finalmodel))
which( lev >leverageTreshold)
```


**55,56,57,58,59,60,72,157 observations are extreme values for the predictor variables for certain rats**


To find extreme residuals,we have to find value more than $1.96*\hat{\sigma}$.

```{r}
which(abs(resid(finalmodel)) > 1.959964 * summary(finalmodel)$sigma)
```



 14,17,18,72,77,90,96,142,158 extreme residuals


if Cook's distance is over the 50% percentile of an $F_{p,n-p}$-distribution, it is extreme Cook's distance



```{r}
cook <- cooks.distance(finalmodel)
plot(cook,type='h')

fquantile <- qf(0.5,df1 = p,df2 = n - p)
cook < fquantile
which(cook > fquantile)
```



**Fortunately, there is no extreme observations in this model base on the Cook's distance. also, Cook's distance well distributed under approximately 0.1**


we have to conduct DFBETA's to find extreme influence coefficient. a value for DFBETAS is consider to be extreme influence if it surpasses $\frac{2}{\sqrt{n}}$ for large datasets. for small and moderate sized datasets, a value is considered to be extreme if it surpasses 1. our datasets can be considered to large datasets



```{r}
dfb <- dfbetas(finalmodel)
dfb > 2 / sqrt(n)
which(rowSums(dfb > 2 / sqrt(n))!=0)
```



**2,7,9,11,12,14,16,17,20,24,25,31,37,40,47,52 and 56** observations are extreme values for **DFBETA**.
&nbsp;
**2,7,9,11,12,14,16,17,20,24,25,31,37,40,47,52 and 56** observations are extreme values by **large influence** on all the fitted values and coefficients.

&nbsp;
&nbsp;

#### **In sum up, 2,7,9,11,12,14,16,17,18,20,24,25,31,37,40,47,52,55,56,57,58,59,60,72,77,90,96,142,157 and 158 observations are extreme values.**

&nbsp;
&nbsp;
&nbsp;

--------------------------------------------------------------------------------


### **Results**\

> Describe the results as if you would write the results section of a paper.

Using forward building building, 4 different successive models were produced. They are defined below as model1, model2, model3, and finalmodel.


#### **All the successive models**

##### **model1 <- lm(Weightslog ~ Widthlog)**\

##### **model2 <- lm(Weightslog ~ Widthlog + Heightlog)**\

##### **model3 <- lm(Weightslog ~ Widthlog + Heightlog + Length2log)**\

##### **finalmodel <- lm(Weightslog ~ Widthlog + Heightlog + Length2log + factor(Species))**\

our final model has 158 observations, and dependent variable is Weightslog[-41]. it has results shown below.

$F(9,148) = 3766.25, p = 0.00$\
$R² = 1.00$\
$Adj. R² = 1.00$

```{r, include=FALSE}
library(sjPlot)
library(jtools)
library(car)

fish <- read.csv("Fish.csv",sep = ",")
attach(fish)

Weightslog <- log(Weight)
Widthlog <- log(Width)
Heightlog <- log(Height)
Length2log <- log(Length2)

finalmodel <- lm(Weightslog[-41] ~ Widthlog[-41] + Heightlog[-41] + Length2log[-41] + factor(Species[-41]))
```


```{r}
summ(finalmodel)
qqPlot(finalmodel)
plot_model(finalmodel, vline.color = "red", show.values = TRUE, sort.est = TRUE)

```



> Create a table containing an overview of the selected regression coefficients in each of the successive models, including their t-value and p-value.

tab_model() function was used to make a table that shows the t-value and p-value of each predictors for different models. In order to use tab_model() function, three packages have to be called which are sjPlot, sjmisc, and sjlabelled. By altering different parameters in the tab_model() function, table name, row name, and column name were defined. Also, only preferred values were selected to show in the table.


```{r, include=FALSE}
library(sjPlot)
library(sjmisc)
library(sjlabelled)

fish <- read.csv("Fish.csv",sep = ",")
attach(fish)

Weightslog <- log(Weight)
Widthlog <- log(Width)
Heightlog <- log(Height)
Length2log <- log(Length2)

model1 <- lm(Weightslog[-41] ~ Widthlog[-41])
model2 <- lm(Weightslog[-41] ~ Widthlog[-41] + Heightlog[-41])
model3 <- lm(Weightslog[-41] ~ Widthlog[-41] + Heightlog[-41] + Length2log[-41])
finalmodel <- lm(Weightslog[-41] ~ Widthlog[-41] + Heightlog[-41] + Length2log[-41] + factor(Species[-41]))
```


```{r}
tab_model(title = "Table 1: Comparsion of t-value and p-value between the successive models. (Note: All the predictors are in log form)", model1, model2, model3, finalmodel, pred.labels = c("(Intercept)", "Width", "Height", "Length2", "Parkki", "Perch", "Pike", "Roach", "Smelt", "Whitefish"), dv.labels = c("Model 1", "Model 2", "Model 3", "Final model"), show.stat = TRUE, show.ci = FALSE, show.est = TRUE, string.stat = "t-value",  string.p = "p-value")

```

Describe model
Table 1 shows a summary of all the regression coefficients for the preferred models. It also shows the t-values and p-values for each multivariate predictors within the model.

The model was constructed exhaustively with a forward model approach, which starts from the empty model and the new predictors were added based on its largest t-value, with their p-values being less than 0.05. This is to verify if certain predictors in the model were significantly associated with the outcome.


> Create a table for the successive models. Include, for each model, the $R^2$-value and Residual Sum of Errors (RSE).

To create a table comparing R2 and RSE values for different models, kable() function was used. In order to use kable() function, it requires knitr and kableExtra packages. A table containing all the information (name of the model, R2 for each successive models, and RSE for each successive models) was created beforehand, and using kable() function, created-table is decorated.


```{r}
# Making a table comparing models with R-squared value and Residual Sum of Errors (RSE).
allmodel_name <- c("Width", "Width + Height", "Width + Height + Length2", "Width + Height + Length2 + Species factor")

allmodel_r_squared <- c(summary(model1)$r.squared, summary(model2)$r.squared, summary(model3)$r.squared, summary(finalmodel)$r.squared)

allmodel_RSE <- c(sum(residuals(model1)^2), sum(residuals(model2)^2),sum(residuals(model3)^2), sum(residuals(finalmodel)^2))

table3 <- data.frame("Model" = allmodel_name, "R-squared" = allmodel_r_squared, "RSE" = allmodel_RSE)

library(knitr)
library(kableExtra)

kable(table3, align = c('l', 'c', 'c'), caption = "Table 2: Comparison of R-squared values and RSE for the successive models") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = T) %>%
  footnote(general = "All the predictors are tranformed in logarithmic form.")

```


> Include a figure inspecting the relation between the predicted and measured weight values.

Different Species are represented by different colors:

The red dots represent Bream.

The orange dots represent Roach.

The yellow dots represent Whitefish.

The green dots represent Parkki.

The blue dots represent Perch.

The navy dots represent Pike.

The purple dots represent Smelt.


```{r}
plot(Weightslog[-41], predict(finalmodel),col= my_cols, pch =16, main =  "Plot 1: Predicted log(Weights) vs Measured log(Weights)", xlab = "Measured log(Weights)", ylab = "Predicted log(Weights)")
abline(a=0,b=1)
```

Plot 1 is created with measured log(Weights) value and predicted log(Weights) value where the predictions are made using the final model. A linear line is also created that passes through the points. From the plot, we can assume that our final model predicts log(Weights) values well because most of the points are on the line meaning that the predicted log(Weights) values are almost equal to the measured log(Weights) values. Also, the predictions are made with high accuracy regardless on different Species. This can be seen as all the dots in different colors (red, orange, yellow, green, blue, purple, black) are near the linear line.